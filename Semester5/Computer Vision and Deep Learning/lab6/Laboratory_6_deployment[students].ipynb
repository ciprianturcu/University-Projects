{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hXhoiRUvnHJ"
      },
      "source": [
        "# Computer vision and deep learning - Laboratory 6\n",
        "\n",
        "In this last laboratory, we will switch our focus from implementing and training neural networks to developing a machine learning application.\n",
        "More specifically you will learn how you can convert your saved torch model into a more portable format using torch script and how you can create a simple demo application for your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v_8pVcLUMVQB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (4.11.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (5.2.0)\n",
            "Requirement already satisfied: fastapi in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.105.0)\n",
            "Requirement already satisfied: ffmpy in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.7.3 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.7.3)\n",
            "Requirement already satisfied: httpx in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.20.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (3.8.1)\n",
            "Requirement already satisfied: numpy~=1.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (1.26.1)\n",
            "Requirement already satisfied: orjson~=3.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: packaging in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (10.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (2.5.2)\n",
            "Requirement already satisfied: pydub in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.9 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (4.8.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio) (0.25.0)\n",
            "Requirement already satisfied: fsspec in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio-client==0.7.3->gradio) (2023.10.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from gradio-client==0.7.3->gradio) (11.0.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.20.0)\n",
            "Requirement already satisfied: toolz in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from fastapi->gradio) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: certifi in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from httpx->gradio) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: idna in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in d:\\university-projects\\semester5\\computer vision and deep learning\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torchscript (from versions: none)\n",
            "ERROR: No matching distribution found for torchscript\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install gradio\n",
        "%pip install torchscript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8spOviRwB3Uz"
      },
      "source": [
        "## Converting your model into portable TorchScript binaries\n",
        "\n",
        "\n",
        "``TorchScript`` allows you to create serializable and optimizable models from PyTorch code and then use them in a process where there is no Python dependency.\n",
        "\n",
        "\n",
        "When deploying our module in production systems, we might need to run the model using another programming language (not Python) and even on mobile or embedded devices. In addition, we need a more lightweight environment than the development one.\n",
        "\n",
        "\n",
        "Until now, when training a model we've saved checkpoints and reloaded the weights when needed into the development environment. As the name suggests, the checkpoints contain additional information (such as optimizer states) which allows you to resume the training process. However, all this information is not required during inference.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "``Torchscript`` allows you to create a lightweight and independent model artifact suitable for runtime via two different techniques: scripting and tracing. They are both used to convert a PyTorch model into a more optimized or deployable form.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Tracing involves capturing a model's execution trace by passing example inputs through the model and recording the operations executed. This creates a TorchScript representation of the model based on the traced operations. However, tracing might not capture all dynamic aspects of the model, especially if the model's behavior changes dynamically based on input data or control flow operations. Tracing is more focused on capturing the specific operations executed with example inputs, which might be more efficient but might not cover all dynamic behaviors of complex models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Scripting, on the other hand, refers to converting a PyTorch model (built using PyTorch's dynamic computation graph with Python control flow, such as loops and if statements) into a TorchScript. This involves representing the model as a static computation graph that can be executed independently of Python. Scripting allows the model to be saved and run in environments where a Python interpreter might not be available. Scripting captures the entire model logic and can handle more complex models with Pythonic control flow, making it more flexible but potentially more complex.\n",
        "\n",
        "\n",
        "Both techniques aim to transform PyTorch models into TorchScript representations, making them efficient for deployment in various environments or for optimized execution, albeit with different approaches. The choice between scripting and tracing depends on the specific use case, model complexity, and deployment requirements.\n",
        "\n",
        "You can check out the [documentation](https://pytorch.org/docs/stable/jit.html) for further details on ``TorchScript``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26fLZ04xnXyA"
      },
      "source": [
        "Below you have an example that demonstrates the conversion of a pre-trained ResNet-18 model from torchvision into a TorchScript and then loading and using the saved TorchScript model for inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P2gPwGpavl7C"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet18(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
            "File \u001b[1;32md:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32md:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\torchvision\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[1;32md:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\torchvision\\models\\convnext.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n",
            "File \u001b[1;32md:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\torchvision\\ops\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_register_onnx_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _register_custom_op\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboxes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     batched_nms,\n\u001b[0;32m      4\u001b[0m     box_area,\n\u001b[0;32m      5\u001b[0m     box_convert,\n\u001b[0;32m      6\u001b[0m     box_iou,\n\u001b[0;32m      7\u001b[0m     clip_boxes_to_image,\n\u001b[0;32m      8\u001b[0m     complete_box_iou,\n\u001b[0;32m      9\u001b[0m     distance_box_iou,\n\u001b[0;32m     10\u001b[0m     generalized_box_iou,\n\u001b[0;32m     11\u001b[0m     masks_to_boxes,\n\u001b[0;32m     12\u001b[0m     nms,\n\u001b[0;32m     13\u001b[0m     remove_small_boxes,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mciou_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m complete_box_iou_loss\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeform_conv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deform_conv2d, DeformConv2d\n",
            "File \u001b[1;32md:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\torchvision\\ops\\boxes.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_box_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _box_cxcywh_to_xyxy, _box_xywh_to_xyxy, _box_xyxy_to_cxcywh, _box_xyxy_to_xywh\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _upcast\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnms\u001b[39m(boxes: Tensor, scores: Tensor, iou_threshold: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    Performs non-maximum suppression (NMS) on the boxes according\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    to their intersection-over-union (IoU).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m        by NMS, sorted in decreasing order of scores\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Create a sample input tensor (change according to your model's input requirements)\n",
        "example_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Script the model\n",
        "scripted_model = torch.jit.script(model)\n",
        "\n",
        "# Save the scripted model to a file\n",
        "scripted_model.save(\"scripted_resnet18.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBYTvmFYr9Fz"
      },
      "source": [
        "The main steps of the process are:\n",
        "- load the pre-trained model and set it to evaluation mode with model.eval().\n",
        "- create a sample input tensor (example_input) that matches the expected input shape of the model.\n",
        "- use ```torch.jit.script()``` to convert the model into a TorchScript representation.\n",
        "- save the scripted model to a file using ```scripted_model.save()``` for later use or deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eMTs3vbsV56"
      },
      "source": [
        "Now, let's see how you can use the scripted model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TO58HkOtsZXx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: tabby, Probability: 0.7649\n",
            "Prediction: tiger cat, Probability: 0.1408\n",
            "Prediction: Egyptian cat, Probability: 0.0876\n",
            "Prediction: Persian cat, Probability: 0.0024\n",
            "Prediction: lynx, Probability: 0.0013\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import torchvision\n",
        "from torchvision.models import  ResNet18_Weights\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "# Load the saved TorchScript model\n",
        "model = torch.jit.load(\"scripted_resnet18.pt\")\n",
        "\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "image_url = 'https://images.unsplash.com/photo-1611267254323-4db7b39c732c?q=80&w=1000&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8M3x8Y3V0ZSUyMGNhdHxlbnwwfHwwfHx8MA%3D%3D'\n",
        "response = requests.get(image_url)\n",
        "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "\n",
        "input_tensor = preprocess(image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # run the scripted model\n",
        "    output = model(input_batch)\n",
        "\n",
        "weights = ResNet18_Weights.DEFAULT\n",
        "\n",
        "class_names = weights.meta[\"categories\"]\n",
        "\n",
        "# Get the top 5 predictions\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "\n",
        "# Display top 5 predicted classes and their probabilities\n",
        "for i in range(top5_prob.size(0)):\n",
        "    class_idx = top5_catid[i].item()\n",
        "    print(f\"Prediction: {class_names[class_idx]}, Probability: {top5_prob[i].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaEEfd1NOxyk"
      },
      "source": [
        "Optionally, you can also save the torchscript binary into ```wandb```. In this way, you will have a connection link between the model that is running in production and the training runs that you logged during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-btbzsULYXM"
      },
      "source": [
        "# Creating a simple UI with gradio\n",
        "\n",
        "\n",
        "[Gradio](https://www.gradio.app/docs/interface) is an open-source Python library used for creating customizable UI components for machine learning models with just a few lines of code. It greatly simplifies the process of building web-based interfaces to interact with ML models without requiring extensive knowledge of web development and allows you to quickly build an MVP and get feedback from the users.\n",
        "\n",
        "\n",
        "To get an application running, you just need to specify three parameters:\n",
        "1. the function to wrap the interface around.\n",
        "2. what are the desired input components?\n",
        "3. what are the desired output components?\n",
        "\n",
        "\n",
        "This is achieved through the ``gradio.Interface`` class, the central component in gradio, responsible for creating the user interface for your machine learning model.\n",
        "\n",
        "\n",
        "```\n",
        "import gradio as gr\n",
        "demo = gr.Interface(fn=image_classifier,\n",
        "                    inputs=\"image\",\n",
        "                    outputs=\"label\")\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Once you've defined the gr.Interface, the launch() method is used to start the interface, making it accessible through a web browser.\n",
        "\n",
        "\n",
        "```\n",
        "demo.launch()\n",
        "```\n",
        "\n",
        "\n",
        "When the launch method is called, ```gradio``` launches a simple web server that serves the demo. If you specify ```share=True``` when calling the launch function, ```gradio``` will create a public link Can also be used to create a public link used by anyone to access the demo from their browser.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXflaukmmWPh"
      },
      "source": [
        "## Simple UI for image classification in gradio\n",
        "\n",
        "Below you have an example of how you could use ```gradio``` to create a simple UI for an image classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fpXMo37vMUYL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def softmax(x):\n",
        "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())\n",
        "\n",
        "\n",
        "def classify_image(img):\n",
        "    # TODO run a classification model to get the class scores\n",
        "    prediction = softmax(np.random.randn(10, ))\n",
        "    confidences = {CLASSES[i]: float(prediction[i]) for i in range(len(CLASSES))}\n",
        "    return confidences\n",
        "\n",
        "ui = gr.Interface(fn=classify_image,\n",
        "             inputs=gr.Image(),\n",
        "             outputs=gr.Label(num_top_classes=3),\n",
        "             # TODO replace example1.png example2.png with some images from your device\n",
        "            examples=['D:\\\\University-Projects\\\\Semester5\\\\Computer Vision and Deep Learning\\\\lab6\\\\elephant.jpg']\n",
        "          )\n",
        "ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKXtFC0_hkcm"
      },
      "source": [
        "## Accessing the webcam with gradio\n",
        "\n",
        "In the example below, you have an example in which you take the input images from your webcam.\n",
        "The function wrapped by gradio uses a mask to blur the input image outside that mask. If you plan to do background blurring, the mask could be the segmentation mask predicted by your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HzC5frqLhk1I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/queueing.py\", line 489, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/blocks.py\", line 1533, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/blocks.py\", line 1151, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/utils.py\", line 678, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/var/folders/0l/bbl3jypx4yqgrjtryn71p_h80000gn/T/ipykernel_34691/191278018.py\", line 6, in blur_background\n",
            "    input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "cv2.error: OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/queueing.py\", line 489, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/blocks.py\", line 1533, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/blocks.py\", line 1151, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/soranaaa/Documents/ubb/third-year/compvis/lab1/env/lib/python3.11/site-packages/gradio/utils.py\", line 678, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/var/folders/0l/bbl3jypx4yqgrjtryn71p_h80000gn/T/ipykernel_34691/191278018.py\", line 6, in blur_background\n",
            "    input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "cv2.error: OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "def blur_background(input_image):\n",
        "    input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Generate a blank mask\n",
        "    # TODO your code here: call a segmentation model to get predicted mask\n",
        "    mask = np.zeros_like(input_image)\n",
        "\n",
        "    # for demo purposes, we are going to create a random segmentation mask\n",
        "    #  just a circular blob centered in the middle of the image\n",
        "    center_x, center_y = mask.shape[1] // 2, mask.shape[0] // 2\n",
        "    cv2.circle(mask, (center_x, center_y), 100, (255, 255, 255), -1)\n",
        "\n",
        "    # Convert the mask to grayscale\n",
        "    mask_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "    mask_gray = mask_gray[:,:,np.newaxis]\n",
        "\n",
        "    # apply a strong Gaussian blur to the areas outside the mask\n",
        "    blurred = cv2.GaussianBlur(input_image, (51, 51), 0)\n",
        "    result = np.where(mask_gray, input_image, blurred)\n",
        "\n",
        "    # Convert the result back to RGB format for Gradio\n",
        "    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "    return result\n",
        "\n",
        "\n",
        "ui = gr.Interface(\n",
        "    fn=blur_background,\n",
        "    inputs=gr.Image(sources=[\"webcam\"]),\n",
        "    outputs=\"image\",\n",
        "    title=\"Image segmentation demo!\"\n",
        "\n",
        ")\n",
        "ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhaGtNlJj1Wr"
      },
      "source": [
        "## Laboratory assignment\n",
        "\n",
        "\n",
        "Now you have all the knowledge required to build your own ML semantic segmentation application.\n",
        "\n",
        "\n",
        "1. First use ```torchscript``` to obtain a model binary.\n",
        "2. Using gradio, create a simple application that uses the semantic segmentation that you developed. Feel free to define the scope and the functional requirements of your app.\n",
        "3. __[Optional, independent work]__ Use a serverless cloud function on [AWS Lambda](https://aws.amazon.com/lambda/) (this requires an account on Amazon AWS and you need to provide the details of a credit card) to run the prediction and get the results.\n",
        "\n",
        "\n",
        "Congratulations, you've just completed all the practical work for Computer Vision and Deep Learning!\n",
        "May your data always be clean, your models accurate, and your code bug-free!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        # Encoder\n",
        "        self.ec11 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.ec12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.ec21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.ec22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.ec31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.ec32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.mp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.ec41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.ec42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.mp4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.ec51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
        "        self.ec52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dc41 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
        "        self.dc42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dc31 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
        "        self.dc32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dc21 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.dc22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dc11 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.dc12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, 3, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        ec11_out = nn.ReLU(inplace=True)(self.ec11(x))\n",
        "        ec12_out = nn.ReLU(inplace=True)(self.ec12(ec11_out))\n",
        "        pool1_out = self.mp1(ec12_out)\n",
        "\n",
        "        ec21_out = nn.ReLU(inplace=True)(self.ec21(pool1_out))\n",
        "        ec22_out = nn.ReLU(inplace=True)(self.ec22(ec21_out))\n",
        "        pool2_out = self.mp2(ec22_out)\n",
        "\n",
        "        ec31_out = nn.ReLU(inplace=True)(self.ec31(pool2_out))\n",
        "        ec32_out = nn.ReLU(inplace=True)(self.ec32(ec31_out))\n",
        "        pool3_out = self.mp3(ec32_out)\n",
        "\n",
        "        ec41_out = nn.ReLU(inplace=True)(self.ec41(pool3_out))\n",
        "        ec42_out = nn.ReLU(inplace=True)(self.ec42(ec41_out))\n",
        "        pool4_out = self.mp4(ec42_out)\n",
        "\n",
        "        ec51_out = nn.ReLU(inplace=True)(self.ec51(pool4_out))\n",
        "        ec52_out = nn.ReLU(inplace=True)(self.ec52(ec51_out))\n",
        "\n",
        "        #Decoder \n",
        "        upconv4_out = self.upconv4(ec52_out)\n",
        "        cat4 = torch.cat([ec42_out, upconv4_out], dim=1) \n",
        "        dc41_out = nn.ReLU(inplace=True)(self.dc41(cat4))\n",
        "        dc42_out = nn.ReLU(inplace=True)(self.dc42(dc41_out))\n",
        "\n",
        "        upconv3_out = self.upconv3(dc42_out)\n",
        "        cat3 = torch.cat([ec32_out, upconv3_out], dim=1) \n",
        "        dc31_out = nn.ReLU(inplace=True)(self.dc31(cat3))\n",
        "        dc32_out = nn.ReLU(inplace=True)(self.dc32(dc31_out))\n",
        "\n",
        "        upconv2_out = self.upconv2(dc32_out)\n",
        "        cat2 = torch.cat([ec22_out, upconv2_out], dim=1) \n",
        "        dc21_out = nn.ReLU(inplace=True)(self.dc21(cat2))\n",
        "        dc22_out = nn.ReLU(inplace=True)(self.dc22(dc21_out))\n",
        "        \n",
        "        upconv1_out = self.upconv1(dc22_out)\n",
        "        cat1 = torch.cat([ec12_out, upconv1_out], dim=1) \n",
        "        dc11_out = nn.ReLU(inplace=True)(self.dc11(cat1))\n",
        "        dc12_out = nn.ReLU(inplace=True)(self.dc12(dc11_out))\n",
        "\n",
        "        final_out = self.final_conv(dc12_out)\n",
        "\n",
        "        return final_out\n",
        "\n",
        "model = UNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: ''",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model_state_dict = model.state_dict()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# for key in checkpoint.keys():\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     if key in model_state_dict:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#         model_state_dict[key] = checkpoint[key]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(model_state_dict)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# model.eval()  \u001b[39;00m\n",
            "File \u001b[1;32md:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[1;32md:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[1;32md:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load(\"\")\n",
        "model.load_state_dict(checkpoint)\n",
        "# model_state_dict = model.state_dict()\n",
        "# for key in checkpoint.keys():\n",
        "#     if key in model_state_dict:\n",
        "#         model_state_dict[key] = checkpoint[key]\n",
        "# model.load_state_dict(model_state_dict)\n",
        "# model.eval()  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as v2\n",
        "\n",
        "transformations = v2.Compose([\n",
        "    v2.Resize(256),\n",
        "    v2.CenterCrop(224),\n",
        "    v2.ToTensor(),\n",
        "])\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = Image.fromarray(image)\n",
        "\n",
        "    image = transformations(image)\n",
        "\n",
        "    image = image.unsqueeze(0)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_segmentation(model, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image)\n",
        "    \n",
        "    _, predicted_mask = torch.max(prediction, dim=1)\n",
        "    predicted_mask = predicted_mask.squeeze(0)  \n",
        "\n",
        "    return predicted_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_segmentation(predicted_mask):\n",
        "    label_colors = np.array([(255, 0, 0),  \n",
        "                             (0, 255, 0), \n",
        "                             (0, 0, 255)])\n",
        "\n",
        "    colored_mask = label_colors[predicted_mask]\n",
        "\n",
        "    return colored_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\University-Projects\\Semester5\\Computer Vision and Deep Learning\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "model = UNet()\n",
        "model.load_state_dict(torch.load(\"model_epoch_10.pth\")) \n",
        "\n",
        "def test(input_image):\n",
        "    print(input_image)\n",
        "    return input_image\n",
        "\n",
        "def inference(input_image):\n",
        "\n",
        "    print(0)\n",
        "    input_image = preprocess_image(input_image)\n",
        "\n",
        "    print(1)\n",
        "    predicted_mask = predict_segmentation(model, input_image)\n",
        "    \n",
        "    print(2)\n",
        "    segmentation_image = visualize_segmentation(predicted_mask.numpy())\n",
        "\n",
        "    return segmentation_image\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=inference,\n",
        "    inputs=gr.Image(sources=[\"webcam\"]),\n",
        "    outputs=\"image\",\n",
        "    live=True,\n",
        "    title=\"Real-time Image Segmentation (Background, Hair, Face)\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
